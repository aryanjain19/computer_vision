{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "c0f70214c0dd213f07f54ee5d6e0ea644bdbba35113c9bfe8aaa0d1db03ad5dd"
    },
    "colab": {
      "name": "CV_project_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3ESR4xS9IFv"
      },
      "source": [
        "This model predicts the numbers via hand gestures using Deep Learning, CNN, OpenCV, Skimage, Tensorflow. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-06-28T10:36:28.496880Z",
          "iopub.execute_input": "2021-06-28T10:36:28.497202Z",
          "iopub.status.idle": "2021-06-28T10:36:35.037132Z",
          "shell.execute_reply.started": "2021-06-28T10:36:28.497170Z",
          "shell.execute_reply": "2021-06-28T10:36:35.036368Z"
        },
        "trusted": true,
        "id": "AKeDSzrrT941"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import regularizers\n",
        "\n",
        "import os, glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import skimage\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "from skimage.feature import hog\n",
        "from skimage import io, transform\n",
        "import time \n",
        "from cv2 import IMREAD_GRAYSCALE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:36:39.999285Z",
          "iopub.execute_input": "2021-06-28T10:36:39.999605Z",
          "iopub.status.idle": "2021-06-28T10:36:40.488084Z",
          "shell.execute_reply.started": "2021-06-28T10:36:39.999578Z",
          "shell.execute_reply": "2021-06-28T10:36:40.487066Z"
        },
        "trusted": true,
        "id": "WLuc4vhuT947"
      },
      "source": [
        "train_images = glob.glob(\"../input/fingers/train/*.png\")\n",
        "test_images = glob.glob(\"../input/fingers/test/*.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ODidCy9xXX"
      },
      "source": [
        "Load the train and test images for Deep Learning Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:36:42.309052Z",
          "iopub.execute_input": "2021-06-28T10:36:42.309386Z",
          "iopub.status.idle": "2021-06-28T10:39:09.872499Z",
          "shell.execute_reply.started": "2021-06-28T10:36:42.309354Z",
          "shell.execute_reply": "2021-06-28T10:39:09.871496Z"
        },
        "trusted": true,
        "id": "28erqn6jT948"
      },
      "source": [
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "    \n",
        "for img in train_images:\n",
        "    img_read = io.imread(img)\n",
        "\n",
        "    img_read = transform.resize(img_read, (128,128), mode = 'constant')#resize images to make them of same size\n",
        "    X_train.append(img_read)\n",
        "    y_train.append(img[-6:-4])\n",
        "    \n",
        "for img in test_images:\n",
        "    img_read = io.imread(img)\n",
        "    img_read = transform.resize(img_read, (128,128), mode = 'constant')\n",
        "    X_test.append(img_read)\n",
        "    y_test.append(img[-6:-4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:39:25.137987Z",
          "iopub.execute_input": "2021-06-28T10:39:25.138329Z",
          "iopub.status.idle": "2021-06-28T10:39:25.143568Z",
          "shell.execute_reply.started": "2021-06-28T10:39:25.138296Z",
          "shell.execute_reply": "2021-06-28T10:39:25.142606Z"
        },
        "trusted": true,
        "id": "ny7aMDlHT95A"
      },
      "source": [
        "X_train = np.expand_dims(X_train, axis=3)\n",
        "X_test = np.expand_dims(X_test, axis=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0mMfUis95br"
      },
      "source": [
        "Classes for hand gestures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:39:28.671753Z",
          "iopub.execute_input": "2021-06-28T10:39:28.672082Z",
          "iopub.status.idle": "2021-06-28T10:39:28.677225Z",
          "shell.execute_reply.started": "2021-06-28T10:39:28.672052Z",
          "shell.execute_reply": "2021-06-28T10:39:28.676184Z"
        },
        "trusted": true,
        "id": "ZEMvCABET95A"
      },
      "source": [
        "label_to_int={\n",
        "    '0R' : 0,\n",
        "    '1R' : 1,\n",
        "    '2R' : 2,\n",
        "    '3R' : 3,\n",
        "    '4R' : 4,\n",
        "    '5R' : 5,\n",
        "    '0L' : 6,\n",
        "    '1L' : 7,\n",
        "    '2L' : 8,\n",
        "    '3L' : 9,\n",
        "    '4L' : 10,\n",
        "    '5L' : 11\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:39:32.032313Z",
          "iopub.execute_input": "2021-06-28T10:39:32.032644Z",
          "iopub.status.idle": "2021-06-28T10:39:32.050432Z",
          "shell.execute_reply.started": "2021-06-28T10:39:32.032614Z",
          "shell.execute_reply": "2021-06-28T10:39:32.049374Z"
        },
        "trusted": true,
        "id": "DWKttAR-T95B"
      },
      "source": [
        "temp = []\n",
        "for label in y_train:\n",
        "    temp.append(label_to_int[label])\n",
        "y_train = temp.copy()\n",
        "\n",
        "temp = []\n",
        "for label in y_test:\n",
        "    temp.append(label_to_int[label])\n",
        "y_test = temp.copy()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes = 12)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes = 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfgEmj4b-nsR"
      },
      "source": [
        "Define the architecture of the Deep Learning model using Convolutional Layers.\n",
        "Conv2D and MaxPooling layers are used from keras. The model used is Sequential. Finally the model is complied to train it in next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:39:35.466719Z",
          "iopub.execute_input": "2021-06-28T10:39:35.467033Z",
          "iopub.status.idle": "2021-06-28T10:39:37.919430Z",
          "shell.execute_reply.started": "2021-06-28T10:39:35.467005Z",
          "shell.execute_reply": "2021-06-28T10:39:37.918514Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kWkZ8gcT95C",
        "outputId": "63318e91-6e99-4fc8-f754-a700d043052e"
      },
      "source": [
        "weight_decay = 1e-4\n",
        "\n",
        "num_classes = 12\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64, (4,4), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(128,128,1)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (4,4), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2)) #Dropout layer is added to prevent overfitting\n",
        " \n",
        "model.add(Conv2D(128, (4,4), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (4,4), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (4,4), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (4,4), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=\"linear\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "#Now compile the model using appropriate loss function and optimizer. \n",
        "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.0003), metrics=['accuracy'])\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 128, 128, 64)      1088      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128, 128, 64)      256       \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 128, 128, 64)      65600     \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 128, 128, 64)      256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64, 64, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 64, 64, 128)       131200    \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 64, 64, 128)       262272    \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 32, 32, 128)       262272    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 32, 32, 128)       262272    \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_11 (Batc (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 32768)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               4194432   \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12)                1548      \n",
            "=================================================================\n",
            "Total params: 5,183,244\n",
            "Trainable params: 5,181,964\n",
            "Non-trainable params: 1,280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L50nlBxQ_c3a"
      },
      "source": [
        "Now fit the train data into compiled model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:39:46.883612Z",
          "iopub.execute_input": "2021-06-28T10:39:46.883930Z",
          "iopub.status.idle": "2021-06-28T10:44:18.512085Z",
          "shell.execute_reply.started": "2021-06-28T10:39:46.883901Z",
          "shell.execute_reply": "2021-06-28T10:44:18.511352Z"
        },
        "trusted": true,
        "id": "1Le_6pgNT95D",
        "outputId": "f8ab2a32-972b-49fb-893a-245f97c1b422"
      },
      "source": [
        "model.fit(x = X_train,y = y_train, batch_size=64, validation_data = (X_test,y_test), epochs = 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "282/282 [==============================] - 58s 192ms/step - loss: 1.7166 - accuracy: 0.7508 - val_loss: 19.2836 - val_accuracy: 0.0833\n",
            "Epoch 2/5\n",
            "282/282 [==============================] - 53s 188ms/step - loss: 0.0571 - accuracy: 0.9993 - val_loss: 0.8843 - val_accuracy: 0.7794\n",
            "Epoch 3/5\n",
            "282/282 [==============================] - 53s 188ms/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 0.0547 - val_accuracy: 0.9997\n",
            "Epoch 4/5\n",
            "282/282 [==============================] - 53s 188ms/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "282/282 [==============================] - 53s 188ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1751c95350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3kJVTwJAT7J"
      },
      "source": [
        "Woah!. We achieved accuracy of 100% :)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:45:06.280009Z",
          "iopub.execute_input": "2021-06-28T10:45:06.280368Z",
          "iopub.status.idle": "2021-06-28T10:45:06.447754Z",
          "shell.execute_reply.started": "2021-06-28T10:45:06.280338Z",
          "shell.execute_reply": "2021-06-28T10:45:06.447006Z"
        },
        "trusted": true,
        "id": "s6aojAwJT95E"
      },
      "source": [
        "model.save('/content/CV_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-06-28T10:45:42.229405Z",
          "iopub.execute_input": "2021-06-28T10:45:42.229837Z",
          "iopub.status.idle": "2021-06-28T10:45:42.556293Z",
          "shell.execute_reply.started": "2021-06-28T10:45:42.229798Z",
          "shell.execute_reply": "2021-06-28T10:45:42.555495Z"
        },
        "trusted": true,
        "id": "x8lEU2ngT95E"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('/content/CV_model.h5') #Load the trained model for use in CV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoZpplpkAyA8"
      },
      "source": [
        "**capture_image** function takes the image of user after 3sec countdown using camera if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1zD4hhPUtyL"
      },
      "source": [
        "def capture_image():\n",
        "    \"\"\"\n",
        "    returns: image after converting into proper format\n",
        "    Suggestion: Function can be modified to provide live feedback also\n",
        "    \"\"\"\n",
        "    vid_cap = cv2.VideoCapture(0)\n",
        "    captured_image = None\n",
        "    timer = time.perf_counter()\n",
        "\n",
        "    if not vid_cap.isOpened():\n",
        "        print(\"Cannot open camera. Exiting..\")\n",
        "        return None\n",
        "    while True:\n",
        "        ret, frame = vid_cap.read()\n",
        "        if not ret:\n",
        "            print(\"Cannot receive frame. Exiting..\")\n",
        "            break\n",
        "        plt.imshow(frame)\n",
        "        captured_image = frame\n",
        "        #captured_image = cv2.cvtColor(captured_image, cv2.COLOR_BGR2GRAY)\n",
        "        if cv2.waitKey(1) == ord('q') or time.perf_counter()- timer > 3:\n",
        "            break\n",
        "    #avg_frame = cv2.boxFilter(captured_image,1,(10,10))\n",
        "    #captured_image = captured_image - avg_frame\n",
        "    vid_cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return captured_image\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_snM21K2BM0y"
      },
      "source": [
        "Previously attempted approach for this model without DL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dq2fnxsUxWd"
      },
      "source": [
        "# def process_and_load_template(image_path, pixelpercell, img_shape):\n",
        "#     \"\"\"\n",
        "#         temp: raw temp as a numpy array\n",
        "#         returns : processes images to get avg images and finally returns hog image and its features\n",
        "#     \"\"\"\n",
        "#     image_feature_list=[]\n",
        "#     hog_image_list=[]\n",
        "#     roots=[]\n",
        "#     filenames=[]\n",
        "#     for root, _, filename in os.walk(image_path):\n",
        "#       if len(filename)>0:\n",
        "#         roots.append(root)\n",
        "#         filenames.append(filename)\n",
        "\n",
        "#     for i in range(1,6):\n",
        "#       function='finger'+'_avg'\n",
        "#       a=eval(function+'(filenames[i-1],img_shape,roots[i-1])')\n",
        "#       # for i in range(0, a.shape[0]):\n",
        "#       #   for j in range(0, a.shape[1]):\n",
        "#       #     if a[i,j] < 150:\n",
        "#       #       a[i,j] = a[i,j]/2\n",
        "#       #     else:\n",
        "#       #       a[i,j] = 200\n",
        "#       plt.imshow(a)\n",
        "#       plt.show()\n",
        "#       (feature, hog_image)=hog(a,pixels_per_cell=(pixelpercell,pixelpercell),visualize=True)\n",
        "#       image_feature_list.append(feature)\n",
        "#       hog_image_list.append(hog_image)\n",
        "#     return hog_image_list,image_feature_list\n",
        "\n",
        "\n",
        "# def finger_avg(filename,img_shape,root):\n",
        "   \n",
        "#     avg_1_finger = np.zeros(img_shape)\n",
        "    \n",
        "#     for file_ in filename:\n",
        "#         image = cv2.imread(os.path.join(root, file_),flags=IMREAD_GRAYSCALE)\n",
        "#         image=cv2.resize(image,(img_shape[1],img_shape[0]))\n",
        "#         avg_1_finger = np.asarray(image)+np.asarray(avg_1_finger)\n",
        "    \n",
        "        \n",
        "#     avg_1_finger = avg_1_finger / len(filename)\n",
        "#     return avg_1_finger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf3rPpZ6BibI"
      },
      "source": [
        "**load_data** function to load test images from dir if capture_image does not work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MSL96JoU1G5"
      },
      "source": [
        "def load_data(data_dir):\n",
        "    \"\"\"\n",
        "        Load the data to be predicted.\n",
        "        Returns: tuple (images, counts)\n",
        "        images: list of images as numpy array\n",
        "        counts: list of finger count for each corresponding image\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    counts = []\n",
        "    for root, _, filenames in os.walk(data_dir):\n",
        "        print(filenames)\n",
        "        for file_ in filenames:\n",
        "            image = cv2.imread(os.path.join(root, file_),flags=IMREAD_GRAYSCALE)\n",
        "            images.append(image)\n",
        "            count = os.path.join(root,file_).split(\"/\").pop(-2)\n",
        "            counts.append(count)\n",
        "\n",
        "    \n",
        "    return (images, counts)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx7dAab6Btbm"
      },
      "source": [
        "**pre_process_image** manipulates the input image to extract important features and remove noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2uKBgzU3hq"
      },
      "source": [
        "def pre_process_image(img,scale = 0.9,minH = 100):\n",
        "    \"\"\"\n",
        "        Manipulate the image accordingly\n",
        "        making pyramids of images with different sizes\n",
        "        Returns: processed_image (pyramid)\n",
        "    \"\"\"\n",
        "    img=cv2.resize(img,(600,500))\n",
        "    fil = cv2.GaussianBlur(img,(5,5),0)\n",
        "    fil = (fil-fil.mean())/fil.std()   # normalisation\n",
        "    pyramid = []   \n",
        "    dst = np.copy(fil)\n",
        "    pyramid = [dst]\n",
        "    while dst.shape[0]>minH:\n",
        "        dst = cv2.resize(dst,(int(dst.shape[0]*scale),int(dst.shape[1]*scale)))\n",
        "        pyramid.append(dst)\n",
        "    return pyramid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpqNiJWJU5TB"
      },
      "source": [
        "# def sliding_win(img, hand_template, winH, winW,stepSize, pixels_per_cell):\n",
        "#     #sliding window to detect location of hands\n",
        "#     # and to make a box\n",
        "#     \"\"\"\n",
        "#     Img: user image after processing\n",
        "#     template: hand template to detect approx pos of hand\n",
        "#     Returns : return co-ordinates of top_left point \n",
        "#     \"\"\"\n",
        "#     H,W=img.shape\n",
        "#     pad_img = np.lib.pad(img,((winH // 2,winH - winH // 2),(winW // 2,winW - winW // 2)),mode='constant')\n",
        "#     response_map = np.zeros((H // stepSize + 1, W // stepSize + 1))\n",
        "#     maxscore=0\n",
        "\n",
        "#     for i in range(0,H+1,stepSize):\n",
        "#       for j in range(0,W+1,stepSize):\n",
        "#         win=pad_img[i:i+winH,j:j+winW]\n",
        "#         hogfeature=hog(win,pixels_per_cell=(pixels_per_cell,pixels_per_cell))\n",
        "#         hogscore=np.dot(hogfeature,hand_template)\n",
        "#         response_map[i//stepSize][j//stepSize]=hogscore\n",
        "#         if hogscore> maxscore:\n",
        "#           maxscore=hogscore\n",
        "#           maxr=i-winH//2\n",
        "#           maxc=j -winW//2\n",
        "#     response_map = cv2.resize(response_map,(img.shape[1],img.shape[0]))\n",
        "#     return (maxscore, maxr, maxc, response_map)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRvlkWZKCDfQ"
      },
      "source": [
        "**sliding_win** slides a fixed size window over the input image and the window is sent to DL model to predict the hand gesture in that window. If no hand is present that window is discarded as we used threshold value on probabilty to filter them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1auOXwLV5Ec"
      },
      "source": [
        "def sliding_win(img, winH, winW,stepSize):\n",
        "    #sliding window to detect location of hands\n",
        "    # and to make a box\n",
        "    \"\"\"\n",
        "    Img: user image after processing\n",
        "    template: hand template to detect approx pos of hand\n",
        "    Returns : return co-ordinates of top_left point \n",
        "    \"\"\"\n",
        "    H,W=img.shape\n",
        "    pad_img = np.lib.pad(img,((winH // 2,winH - winH // 2),(winW // 2,winW - winW // 2)),mode='constant')\n",
        "    \n",
        "    threshold=0.999\n",
        "    maxr=[]\n",
        "    maxc=[]\n",
        "    values=[]\n",
        "    maxscore=0\n",
        "    for i in range(0,H+1,stepSize):\n",
        "      for j in range(0,W+1,stepSize):\n",
        "        win=pad_img[i:i+winH,j:j+winW]\n",
        "        plt.imshow(win)\n",
        "        plt.show()\n",
        "        win = win.reshape(-1, 128, 128, 1)\n",
        "        score=[]\n",
        "        score=model.predict(win)        \n",
        "        n=np.argmax(score)\n",
        "        if score[0,n]>threshold :\n",
        "          maxscore=score[0,n]\n",
        "          values.append(n)\n",
        "          maxr.append(i-WinH//2)\n",
        "          maxc.append(j-WinW//2)\n",
        "  \n",
        "    return (values,maxr, maxc,maxscore)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWuHdlStDG9C"
      },
      "source": [
        "**display_hand** displays the box around detected hand for visual and debugging purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_o4Go6cU8pB"
      },
      "source": [
        "\n",
        "def display_hand(img, winH, winW, top_left_point):\n",
        "    # window output\n",
        "    \"\"\"\n",
        "    returns: nothing, displays a window around selected hand\n",
        "    \"\"\"\n",
        "    (H,W)=top_left_point\n",
        "    boxed_img=cv2.rectangle(img,(W,H),(W+winW,H+winH),(0, 0, 0),2)\n",
        "    plt.imshow(boxed_img)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBa1xwDoU-se"
      },
      "source": [
        "# def template_match(img, template, topleft_x, topleft_y, win_H, win_W, pix_per_cell):\n",
        "#     # Match the template with 5 hand templates\n",
        "#     \"\"\"\n",
        "#     img: user image\n",
        "#     template: matching template\n",
        "#     returns: correlation score\n",
        "#     \"\"\"\n",
        "#     hog_feature = hog(img[topleft_y:topleft_y+win_H,topleft_x:topleft_x+win_W],pixels_per_cell=(pix_per_cell, pix_per_cell))\n",
        "#     hog_score = np.dot(hog_feature, template)\n",
        "    \n",
        "#     return hog_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LITHzxlfVBHQ"
      },
      "source": [
        "WinH=128\n",
        "WinW=128\n",
        "stepSize=32\n",
        "#pixels_per_cell=2\n",
        "\n",
        "# img_dir_path='/Users/hp/OneDrive/Desktop/data_images/image_data'\n",
        "# (images, labels) = load_data(img_dir_path)\n",
        "# a, templates = process_and_load_template(img_dir_path,pixels_per_cell,(WinH,WinW))\n",
        "# for i in range (5):\n",
        "#     plt.imshow(a[i])\n",
        "#     plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IyxbdZ5Derh"
      },
      "source": [
        "**This code controls the flow of the whole model and returns the predicted hand gesture number.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrtSXM_NVE2J"
      },
      "source": [
        "img=capture_image()\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img=transform.resize(img, (500,600), mode = 'constant')\n",
        "\n",
        "current_scale=10/9\n",
        "score = 0\n",
        "max_score=0\n",
        "scale=0\n",
        "rows=[]\n",
        "cols=[]\n",
        "final_rows=[]\n",
        "final_cols=[]\n",
        "values=[]\n",
        "final_values=[]\n",
        "count=0\n",
        "finger_count = 0\n",
        "\n",
        "#loop over all the images of different scales in image pyramid \n",
        "pyramid = pre_process_image(img,0.9,300)\n",
        "\n",
        "for i in pyramid:\n",
        "    # plt.imshow(i)\n",
        "    # plt.show()\n",
        "\n",
        "    current_scale=current_scale*0.9\n",
        "    values,rows,cols,score=sliding_win(i,WinH,WinW,stepSize)\n",
        "\n",
        "    if score>max_score:\n",
        "      max_score=score\n",
        "      final_rows=rows\n",
        "      final_col=cols\n",
        "      final_values=values\n",
        "      scale=current_scale\n",
        "\n",
        "img=transform.resize(img, (int(500*scale),int(600*scale)), mode = 'constant')\n",
        "for i,j in zip(final_rows, final_cols):\n",
        "    display_hand(img,WinH,WinW,(i,j))\n",
        "\n",
        "print(\"Predicted finger count: \",final_values)\n",
        "\n",
        "\n",
        " \n",
        "# #Display the detected hand for debugging \n",
        "# display_hand(img, WinH, WinW, (finalr, finalc))\n",
        "# #Find the best matching template\n",
        "# for t in templates:\n",
        "#     count+=1\n",
        "#     temp_score = template_match(img,t,finalr, finalc,WinH, WinW, 8)\n",
        "#     print(temp_score)\n",
        "#     if temp_score > max_temp_score:\n",
        "#         max_temp_score = temp_score\n",
        "#         finger_count = count\n",
        "#         print(count)\n",
        "\n",
        "# print(\"Predicted finger count: \",finger_count)\n",
        "# #print(\"Actual answer: \", label)\n",
        "# print(\"##########################\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}